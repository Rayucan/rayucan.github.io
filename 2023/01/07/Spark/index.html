<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="Rayucan"><meta name="copyright" content="Rayucan"><meta name="generator" content="Hexo 5.4.0"><meta name="theme" content="hexo-theme-yun"><title>Spark | Rayの云端小屋</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.25/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_dxory92pb0h.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", () => {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
});
</script><link id="light-prism-css" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/themes/prism.css" media="(prefers-color-scheme: light)"><link id="dark-prism-css" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/themes/prism-tomorrow.css" media="(prefers-color-scheme: dark)"><link rel="icon" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#0078E7"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"hostname":"example.com","root":"/","title":"Rayの云端小屋","version":"1.6.1","mode":"light","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}.","hits":"${hits} results found","hits_time":"${hits} results found in ${time} ms"},"anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/utils.js"></script><script src="/js/hexo-theme-yun.js"></script><meta property="og:type" content="article">
<meta property="og:title" content="Spark">
<meta property="og:url" content="http://example.com/2023/01/07/Spark/index.html">
<meta property="og:site_name" content="Rayの云端小屋">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302181721476.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301262248986.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301072233317.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301072236644.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301072241630.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301151551701.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301251709807.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301251731274.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261549630.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261557083.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261558191.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261622672.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261628117.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261712105.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261711887.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261716326.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261653967.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261655898.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301262300141.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301262351595.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091705504.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091705344.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091712456.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091731919.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091732650.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091753853.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091754211.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091741399.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302101720543.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302101721345.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302132144518.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302132144369.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302132144548.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302132214130.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302132156508.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302191540611.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302191540945.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302191542019.png">
<meta property="article:published_time" content="2023-01-07T09:07:48.000Z">
<meta property="article:modified_time" content="2023-02-19T07:50:31.743Z">
<meta property="article:author" content="Rayucan">
<meta property="article:tag" content="BigData">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302181721476.png"></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="Table of Contents"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="Overview"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="Rayucan"><img width="96" loading="lazy" src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/20210515231431.jpg" alt="Rayucan"></a><div class="site-author-name"><a href="/about/">Rayucan</a></div><a class="site-name" href="/about/site.html">Rayの云端小屋</a><sub class="site-subtitle"></sub><div class="site-desciption">Gaze into abyss.</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="Home"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="Archives"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">52</span></a></div><div class="site-state-item"><a href="/categories/" title="Categories"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">3</span></a></div><div class="site-state-item"><a href="/tags/" title="Tags"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">15</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-settings-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/Rayucan" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://wpa.qq.com/msgrd?v=3&amp;uin=775972516&amp;site=qq&amp;menu=yes" title="QQ" target="_blank" style="color:#12B7F5"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-qq-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:775972516@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a></div></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-%E4%B8%8E-Hadoop"><span class="toc-number">1.</span> <span class="toc-text">Spark 与 Hadoop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97"><span class="toc-number">2.</span> <span class="toc-text">Spark 核心模块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Word-Count"><span class="toc-number">3.</span> <span class="toc-text">Word Count</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%9F%E8%83%BD%E5%AE%9E%E7%8E%B0%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">3.1.</span> <span class="toc-text">功能实现流程图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Word-Count-%E4%BC%98%E5%8C%96"><span class="toc-number">4.</span> <span class="toc-text">Word Count 优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B"><span class="toc-number">4.1.</span> <span class="toc-text">改进</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-%E5%AE%9E%E7%8E%B0-Word-Count"><span class="toc-number">5.</span> <span class="toc-text">Spark 实现 Word Count</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83"><span class="toc-number">6.</span> <span class="toc-text">Spark 运行环境</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Local-%E6%A8%A1%E5%BC%8F"><span class="toc-number">6.1.</span> <span class="toc-text">Local 模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Standalone-%E6%A8%A1%E5%BC%8F"><span class="toc-number">6.2.</span> <span class="toc-text">Standalone 模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Yarn-%E6%A8%A1%E5%BC%8F"><span class="toc-number">6.3.</span> <span class="toc-text">Yarn 模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K8S-amp-Mesos-%E6%A8%A1%E5%BC%8F"><span class="toc-number">6.4.</span> <span class="toc-text">K8S &amp; Mesos 模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Windows-%E6%A8%A1%E5%BC%8F"><span class="toc-number">6.5.</span> <span class="toc-text">Windows 模式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84"><span class="toc-number">7.</span> <span class="toc-text">Spark 运行架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-number">7.1.</span> <span class="toc-text">核心组件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Driver"><span class="toc-number">7.1.1.</span> <span class="toc-text">Driver</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Executor"><span class="toc-number">7.1.2.</span> <span class="toc-text">Executor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Master-amp-Worker"><span class="toc-number">7.1.3.</span> <span class="toc-text">Master &amp; Worker</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ApplicationMaster"><span class="toc-number">7.1.4.</span> <span class="toc-text">ApplicationMaster</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="toc-number">7.2.</span> <span class="toc-text">核心概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Executor-%E4%B8%8E-Core"><span class="toc-number">7.2.1.</span> <span class="toc-text">Executor 与 Core</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E5%BA%A6%EF%BC%88Parallelism%EF%BC%89"><span class="toc-number">7.2.2.</span> <span class="toc-text">并行度（Parallelism）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%89%E5%90%91%E6%97%A0%E7%8E%AF%E5%9B%BE%EF%BC%88DAG%EF%BC%89"><span class="toc-number">7.2.3.</span> <span class="toc-text">有向无环图（DAG）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B"><span class="toc-number">8.</span> <span class="toc-text">Spark 核心编程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD"><span class="toc-number">8.1.</span> <span class="toc-text">RDD</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#RDD-%E7%9A%84%E7%89%B9%E7%82%B9"><span class="toc-number">8.1.1.</span> <span class="toc-text">RDD 的特点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RDD-%E4%B8%8E-IO-%E6%B5%81%E7%9A%84%E8%81%94%E7%B3%BB"><span class="toc-number">8.1.2.</span> <span class="toc-text">RDD 与 IO 流的联系</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#IO"><span class="toc-number">8.1.2.1.</span> <span class="toc-text">IO</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#RDD-1"><span class="toc-number">8.1.2.2.</span> <span class="toc-text">RDD</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RDD-%E5%88%9B%E5%BB%BA"><span class="toc-number">8.1.3.</span> <span class="toc-text">RDD 创建</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%8E%E5%86%85%E5%AD%98%E4%B8%AD%E5%88%9B%E5%BB%BA-RDD"><span class="toc-number">8.1.3.1.</span> <span class="toc-text">从内存中创建 RDD</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%8E%E5%A4%96%E5%AD%98%E4%B8%AD%E5%88%9B%E5%BB%BA-RDD"><span class="toc-number">8.1.3.2.</span> <span class="toc-text">从外存中创建 RDD</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%8E%E5%85%B6%E4%BB%96-RDD-%E5%88%9B%E5%BB%BA"><span class="toc-number">8.1.3.3.</span> <span class="toc-text">从其他 RDD 创建</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%9B%B4%E6%8E%A5%E5%88%9B%E5%BB%BA-RDD%EF%BC%88new%EF%BC%89"><span class="toc-number">8.1.3.4.</span> <span class="toc-text">直接创建 RDD（new）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RDD-%E5%88%86%E5%8C%BA"><span class="toc-number">8.1.4.</span> <span class="toc-text">RDD 分区</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD-%E7%AE%97%E5%AD%90"><span class="toc-number">8.2.</span> <span class="toc-text">RDD 算子</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#RDD-%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90"><span class="toc-number">8.2.1.</span> <span class="toc-text">RDD 转换算子</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Value%E7%B1%BB%E5%9E%8B"><span class="toc-number">8.2.1.1.</span> <span class="toc-text">Value类型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8F%8C-Value-%E7%B1%BB%E5%9E%8B"><span class="toc-number">8.2.1.2.</span> <span class="toc-text">双 Value 类型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Key-Value-%E7%B1%BB%E5%9E%8B"><span class="toc-number">8.2.1.3.</span> <span class="toc-text">Key - Value 类型</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RDD-%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90"><span class="toc-number">8.2.2.</span> <span class="toc-text">RDD 行动算子</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD-%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-number">8.3.</span> <span class="toc-text">RDD 序列化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Kryo-%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%86%E6%9E%B6"><span class="toc-number">8.3.1.</span> <span class="toc-text">Kryo 序列化框架</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD-%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="toc-number">8.4.</span> <span class="toc-text">RDD 依赖关系</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="toc-number">8.4.1.</span> <span class="toc-text">依赖关系</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB"><span class="toc-number">8.4.2.</span> <span class="toc-text">血缘关系</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AA%84%E4%BE%9D%E8%B5%96"><span class="toc-number">8.4.3.</span> <span class="toc-text">窄依赖</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%BD%E4%BE%9D%E8%B5%96"><span class="toc-number">8.4.4.</span> <span class="toc-text">宽依赖</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD-%E6%8C%81%E4%B9%85%E5%8C%96"><span class="toc-number">8.5.</span> <span class="toc-text">RDD 持久化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#RDD-Cache-%E7%BC%93%E5%AD%98"><span class="toc-number">8.5.1.</span> <span class="toc-text">RDD Cache 缓存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RDD-CheckPoint-%E6%A3%80%E6%9F%A5%E7%82%B9"><span class="toc-number">8.5.2.</span> <span class="toc-text">RDD CheckPoint 检查点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%93%E5%AD%98%E5%92%8C%E6%A3%80%E6%9F%A5%E7%82%B9%E5%8C%BA%E5%88%AB"><span class="toc-number">8.5.3.</span> <span class="toc-text">缓存和检查点区别</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD-%E5%88%86%E5%8C%BA%E5%99%A8"><span class="toc-number">8.6.</span> <span class="toc-text">RDD 分区器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hash-%E5%88%86%E5%8C%BA"><span class="toc-number">8.6.1.</span> <span class="toc-text">Hash 分区</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Range-%E5%88%86%E5%8C%BA"><span class="toc-number">8.6.2.</span> <span class="toc-text">Range 分区</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD-%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E4%B8%8E%E4%BF%9D%E5%AD%98"><span class="toc-number">8.7.</span> <span class="toc-text">RDD 文件读取与保存</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#text-%E6%96%87%E4%BB%B6"><span class="toc-number">8.7.1.</span> <span class="toc-text">text 文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sequence-%E6%96%87%E4%BB%B6"><span class="toc-number">8.7.2.</span> <span class="toc-text">sequence 文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#object-%E5%AF%B9%E8%B1%A1%E6%96%87%E4%BB%B6"><span class="toc-number">8.7.3.</span> <span class="toc-text">object 对象文件</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="toc-number">8.8.</span> <span class="toc-text">累加器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="toc-number">8.9.</span> <span class="toc-text">广播变量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-SQL"><span class="toc-number">9.</span> <span class="toc-text">Spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-SQL-%E4%B8%8E-Hive"><span class="toc-number">9.1.</span> <span class="toc-text">Spark SQL 与 Hive</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-SQL-%E7%89%B9%E7%82%B9"><span class="toc-number">9.2.</span> <span class="toc-text">Spark SQL 特点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%98%93%E6%95%B4%E5%90%88"><span class="toc-number">9.2.1.</span> <span class="toc-text">易整合</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE%E6%96%B9%E5%BC%8F%E7%BB%9F%E4%B8%80"><span class="toc-number">9.2.2.</span> <span class="toc-text">数据访问方式统一</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E8%BF%9E%E6%8E%A5"><span class="toc-number">9.2.3.</span> <span class="toc-text">标准数据连接</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataFrame"><span class="toc-number">9.3.</span> <span class="toc-text">DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark-1-x"><span class="toc-number">9.3.1.</span> <span class="toc-text">Spark 1.x</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark-2-x"><span class="toc-number">9.3.2.</span> <span class="toc-text">Spark 2.x</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DataFrame-%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96"><span class="toc-number">9.3.3.</span> <span class="toc-text">DataFrame 查询优化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataSet"><span class="toc-number">9.4.</span> <span class="toc-text">DataSet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DataSet-gt-DataFrame"><span class="toc-number">9.4.1.</span> <span class="toc-text">DataSet -&gt; DataFrame</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DataFrame-gt-DataSet"><span class="toc-number">9.4.2.</span> <span class="toc-text">DataFrame -&gt; DataSet</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-Session"><span class="toc-number">9.5.</span> <span class="toc-text">Spark Session</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E6%88%B7%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0-UDF"><span class="toc-number">9.6.</span> <span class="toc-text">用户自定义函数 UDF</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming"><span class="toc-number">10.</span> <span class="toc-text">Spark Streaming</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84"><span class="toc-number">10.1.</span> <span class="toc-text">架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%83%8C%E5%8E%8B%E6%9C%BA%E5%88%B6"><span class="toc-number">10.2.</span> <span class="toc-text">背压机制</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://example.com/2023/01/07/Spark/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="Rayucan"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="Rayの云端小屋"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Spark</h1><div class="post-meta"><div class="post-time" style="display:inline-block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="Created: 2023-01-07 17:07:48" itemprop="dateCreated datePublished" datetime="2023-01-07T17:07:48+08:00">2023-01-07</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-2-line"></use></svg></span> <time title="Modified: 2023-02-19 15:50:31" itemprop="dateModified" datetime="2023-02-19T15:50:31+08:00">2023-02-19</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/%E7%AC%94%E8%AE%B0/" style="--text-color:dimgray" itemprop="url" rel="index"><span itemprop="text">笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/BigData/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">BigData</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#0078E7;"><p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302181721476.png" alt="image-20230218172120285" loading="lazy"></p>
<span id="more"></span>

<h2 id="Spark-与-Hadoop"><a href="#Spark-与-Hadoop" class="headerlink" title="Spark 与 Hadoop"></a>Spark 与 Hadoop</h2><blockquote>
<p>Spark 与 Hadoop 最主要的差别在于多个作业间的数据通信</p>
<p>Spark 多个作业间的数据通信基于内存，而 Hadoop 基于磁盘</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301262248986.png" alt="image-20230126224831357" loading="lazy"></p>
<p>Hadoop: </p>
<p>一次性数据计算</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301072233317.png" alt="image-20230107223303269" loading="lazy"></p>
<p>Hadoop 在处理数据时，先从磁盘中读取数据，然后进行逻辑操作，再将处理结果重新存储到磁盘中</p>
<p>通过磁盘 IO 来进行作业，会消耗大量资源，影响性能</p>
<p>Spark:</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301072236644.png" alt="image-20230107223618591" loading="lazy"></p>
<p>Spark 优化了计算过程，把作业的计算结果放入内存中，这种方式的存取效率较高</p>
<blockquote>
<p>总结：</p>
<p>在绝大多数场景中，Spark 的确比 MapReduce 更有优势</p>
<p>但正是 Spark 基于内存操作，在实际生产环境中，可能会由于内存的限制导致 Job 执行失败，此时 MapReduce 是更好的选择</p>
<p>即 Spark 并不能完全替代 MapReduce</p>
</blockquote>
<h2 id="Spark-核心模块"><a href="#Spark-核心模块" class="headerlink" title="Spark 核心模块"></a>Spark 核心模块</h2><p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301072241630.png" alt="image-20230107224158550" loading="lazy"></p>
<ul>
<li><p>Spark Core：</p>
<p>提供最基础、最核心的功能</p>
</li>
<li><p>Spark SQL：</p>
<p>用于操作结构化数据的组件，通过 Spark SQL，用户可以使用 SQL 语言来查询数据</p>
</li>
<li><p>Spark Streaming：</p>
<p>用于流式计算的组件</p>
</li>
<li><p>Spark MLlib：</p>
<p>用于机器学习的算法库</p>
</li>
<li><p>Spark GraphX：</p>
<p>用于图计算的框架和算法库</p>
</li>
</ul>
<h2 id="Word-Count"><a href="#Word-Count" class="headerlink" title="Word Count"></a>Word Count</h2><pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token keyword">object</span> WordCount <span class="token punctuation">&#123;</span>
    <span class="token keyword">def</span> main<span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
        <span class="token comment">// 建立与 Spark 的连接</span>
        <span class="token keyword">val</span> conf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"WordCount"</span><span class="token punctuation">)</span>
        <span class="token keyword">val</span> sparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>conf<span class="token punctuation">)</span>

        <span class="token comment">// 执行业务操作</span>
        <span class="token comment">// 1 读取文件</span>
        <span class="token keyword">val</span> lines <span class="token operator">=</span> sparkContext<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"data"</span><span class="token punctuation">)</span>
        <span class="token comment">// 2 拆分 "hello world" -> "hello" "world"</span>
        <span class="token keyword">val</span> words <span class="token operator">=</span> lines<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>_<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">// 3 对单词分组 (hello, hello) (world, world)</span>
        <span class="token keyword">val</span> group <span class="token operator">=</span> words<span class="token punctuation">.</span>groupBy<span class="token punctuation">(</span>word <span class="token keyword">=></span> word<span class="token punctuation">)</span>
        <span class="token comment">// 4 转换分组数据 (hello, hello) (world, world) -> (hello, 2) (world, 2)</span>
        <span class="token keyword">val</span> count <span class="token operator">=</span> group<span class="token punctuation">.</span>map <span class="token punctuation">&#123;</span>
            <span class="token keyword">case</span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> list<span class="token punctuation">)</span> <span class="token keyword">=></span> <span class="token punctuation">&#123;</span>
                <span class="token punctuation">(</span>word<span class="token punctuation">,</span> list<span class="token punctuation">.</span>size<span class="token punctuation">)</span>
            <span class="token punctuation">&#125;</span>
        <span class="token punctuation">&#125;</span>
        <span class="token comment">// 5 打印结果</span>
        <span class="token keyword">val</span> array <span class="token operator">=</span> count<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
        array<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
        
        <span class="token comment">// 关闭连接</span>
        sparkContext<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span></code></pre>

<pre class="language-none"><code class="language-none">(hello,4)
(world,2)
(spark,2)</code></pre>



<h3 id="功能实现流程图"><a href="#功能实现流程图" class="headerlink" title="功能实现流程图"></a>功能实现流程图</h3><p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301151551701.png" alt="image-20230115155109532" loading="lazy"></p>
<h2 id="Word-Count-优化"><a href="#Word-Count-优化" class="headerlink" title="Word Count 优化"></a>Word Count 优化</h2><pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token keyword">object</span> WordCount <span class="token punctuation">&#123;</span>
    <span class="token keyword">def</span> main<span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span>

        <span class="token keyword">val</span> conf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"WordCount"</span><span class="token punctuation">)</span>
        <span class="token keyword">val</span> sparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>conf<span class="token punctuation">)</span>


        <span class="token keyword">val</span> lines <span class="token operator">=</span> sparkContext<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"data"</span><span class="token punctuation">)</span>
        

        <span class="token keyword">val</span> words <span class="token operator">=</span> lines<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>_<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
        <span class="token keyword">val</span> wordToOne <span class="token operator">=</span> words<span class="token punctuation">.</span>map<span class="token punctuation">(</span>
            word <span class="token keyword">=></span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        
        <span class="token comment">// 涉及 Scala 的元组引用，_1 引用第一个元素，_2 引用第二个元素</span>
        <span class="token keyword">val</span> wordGroup <span class="token operator">=</span> wordToOne<span class="token punctuation">.</span>groupBy<span class="token punctuation">(</span>
            t <span class="token keyword">=></span> t<span class="token punctuation">.</span>_1
        <span class="token punctuation">)</span>
        
        <span class="token keyword">val</span> wordToCount <span class="token operator">=</span> wordGroup<span class="token punctuation">.</span>map <span class="token punctuation">&#123;</span>
            <span class="token keyword">case</span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> list<span class="token punctuation">)</span> <span class="token keyword">=></span> <span class="token punctuation">&#123;</span>
                list<span class="token punctuation">.</span>reduce<span class="token punctuation">(</span>
                    <span class="token punctuation">(</span>t1<span class="token punctuation">,</span> t2<span class="token punctuation">)</span> <span class="token keyword">=></span> <span class="token punctuation">&#123;</span>
                        <span class="token punctuation">(</span>t1<span class="token punctuation">.</span>_1<span class="token punctuation">,</span> t1<span class="token punctuation">.</span>_2 <span class="token operator">+</span> t2<span class="token punctuation">.</span>_2<span class="token punctuation">)</span>
                    <span class="token punctuation">&#125;</span>
                <span class="token punctuation">)</span>
            <span class="token punctuation">&#125;</span>
        <span class="token punctuation">&#125;</span>
        
        <span class="token keyword">val</span> array <span class="token operator">=</span> wordToCount<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
        array<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
        
        sparkContext<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span></code></pre>

<pre class="language-none"><code class="language-none">(hello,4)
(world,2)
(spark,2)</code></pre>



<h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301251709807.png" alt="image-20230125170914280" loading="lazy"></p>
<h2 id="Spark-实现-Word-Count"><a href="#Spark-实现-Word-Count" class="headerlink" title="Spark 实现 Word Count"></a>Spark 实现 Word Count</h2><pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token keyword">object</span> WordCount <span class="token punctuation">&#123;</span>
    <span class="token keyword">def</span> main<span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span>

        <span class="token keyword">val</span> conf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"WordCount"</span><span class="token punctuation">)</span>
        <span class="token keyword">val</span> sparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>conf<span class="token punctuation">)</span>


        <span class="token keyword">val</span> lines <span class="token operator">=</span> sparkContext<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"data"</span><span class="token punctuation">)</span>
        

        <span class="token keyword">val</span> words <span class="token operator">=</span> lines<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>_<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
        <span class="token keyword">val</span> wordToOne <span class="token operator">=</span> words<span class="token punctuation">.</span>map<span class="token punctuation">(</span>

            word <span class="token keyword">=></span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        
        <span class="token comment">// Spark 提供了更多功能，可以将分组和聚合用一个方法实现</span>
        <span class="token comment">// reduceByKey 就是将相同的 key，对 value 进行聚合</span>
        <span class="token keyword">val</span> wordToCount <span class="token operator">=</span> wordToOne<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span>_ <span class="token operator">+</span> _<span class="token punctuation">)</span>
        
        <span class="token keyword">val</span> array <span class="token operator">=</span> wordToCount<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
        array<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
        
        sparkContext<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span></code></pre>

<pre class="language-none"><code class="language-none">(hello,4)
(world,2)
(spark,2)</code></pre>

<pre class="language-none"><code class="language-none">_ + _ 是由以下两步简化而来（至简原则）
wordToOne.reduceByKey((x, y) &#x3D;&gt; &#123;x + y&#125;)
wordToOne.reduceByKey((x, y) &#x3D;&gt; x + y)</code></pre>

<h2 id="Spark-运行环境"><a href="#Spark-运行环境" class="headerlink" title="Spark 运行环境"></a>Spark 运行环境</h2><h3 id="Local-模式"><a href="#Local-模式" class="headerlink" title="Local 模式"></a>Local 模式</h3><p>在 IDEA 中运行代码的环境称之为开发环境，而 Local 模式与之不同</p>
<p>所谓的 Local 模式，就是不需要其他任何节点资源，就可以在本地执行 Spark 代码的环境，一般用于教学，调试，演示等</p>
<h3 id="Standalone-模式"><a href="#Standalone-模式" class="headerlink" title="Standalone 模式"></a>Standalone 模式</h3><p>Local 本地模式只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行</p>
<p>只使用 Spark 自身节点运行的集群模式，也就是所谓独立部署（Standalone）模式</p>
<p>Spark 的 <code>Standalone</code> 模式体现了经典的 <code>master-slave</code> 模式</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301251731274.png" alt="image-20230125173147108" loading="lazy"></p>
<h3 id="Yarn-模式"><a href="#Yarn-模式" class="headerlink" title="Yarn 模式"></a>Yarn 模式</h3><p>独立部署（Standalone）模式由 Spark 自身提供计算资源，无需其他框架提供资源。</p>
<p>这种方式降低了和其他第三方资源框架的耦合性，独立性非常强</p>
<p>但是 Spark 主要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些</p>
<h3 id="K8S-amp-Mesos-模式"><a href="#K8S-amp-Mesos-模式" class="headerlink" title="K8S &amp; Mesos 模式"></a>K8S &amp; Mesos 模式</h3><p>Mesos 是 Apache 下的开源分布式资源管理框架，它被称为是分布式系统的内核，在 Twitter 得到广泛使用，管理着 Twitter 超过 30,0000 台服务器上的应用部署，但是在国内依然使用着传统的 Hadoop 大数据框架，使用 Mesos 框架的并不多</p>
<h3 id="Windows-模式"><a href="#Windows-模式" class="headerlink" title="Windows 模式"></a>Windows 模式</h3><p>在自己学习时，每次都需要启动虚拟机，启动集群，这是一个比较繁琐的过程， 并且会占大量的系统资源，导致系统执行变慢，不仅仅影响学习效果，也影响学习进度， Spark 提供了可以在 Windows 系统下启动本地集群的方式</p>
<h2 id="Spark-运行架构"><a href="#Spark-运行架构" class="headerlink" title="Spark 运行架构"></a>Spark 运行架构</h2><p>Spark 框架的核心是一个计算引擎，采用了标准的 <code>master-slave</code> 结构</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261549630.png" alt="image-20230126154922451" loading="lazy"></p>
<p>图中展示了 Spark 执行时的基本结构</p>
<p>Driver 表示 master， 负责管理整个集群中的作业任务调度</p>
<p>Executor 表示 slave，负责实际执行任务</p>
<h3 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h3><h4 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h4><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作</p>
<p>Driver 在 Spark 作业执行时主要负责： </p>
<ul>
<li>将用户程序转化为作业（job）</li>
<li>在 Executor 之间调度任务(task)  </li>
<li>跟踪 Executor 的执行情况 </li>
<li>通过 UI 展示查询运行情况 </li>
</ul>
<p>实际上，我们无法准确地描述 Driver 的定义，因为在整个的编程过程中没有看到任何有关 Driver 的字眼</p>
<p>所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为 Driver 类</p>
<h4 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h4><p>Spark Executor 是集群中工作节点（Worker）中一个 JVM 进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立</p>
<p>Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在</p>
<p>如果有 Executor 节点发生了 故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点 上继续运行</p>
<p>Executor 有两个核心功能：</p>
<ul>
<li><p>负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程</p>
</li>
<li><p>它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储</p>
<p>RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算</p>
</li>
</ul>
<h4 id="Master-amp-Worker"><a href="#Master-amp-Worker" class="headerlink" title="Master &amp; Worker"></a>Master &amp; Worker</h4><p>Spark 集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能</p>
<p>所以环境中还有两个核心组件：Master 和 Worker</p>
<p>Master 是一个进程，主要负责资源的调度和分配，进行集群的监控</p>
<p>Worker 也是进程，一个 Worker 运行在集群中的一台服务器上，由 Master 分配资源对数据进行并行的处理和计算</p>
<h4 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h4><p>Hadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包含 <code>ApplicationMaster</code>，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况</p>
<p>即，ResourceManager（资源）和 Driver（计算）之间的<strong>解耦合</strong>靠的就是 <code>ApplicationMaster</code></p>
<h3 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h3><h4 id="Executor-与-Core"><a href="#Executor-与-Core" class="headerlink" title="Executor 与 Core"></a>Executor 与 Core</h4><p>Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门用于计算的节点</p>
<p>在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源</p>
<p>这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261557083.png" alt="image-20230126155734010" loading="lazy"></p>
<h4 id="并行度（Parallelism）"><a href="#并行度（Parallelism）" class="headerlink" title="并行度（Parallelism）"></a>并行度（Parallelism）</h4><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行</p>
<p>注意，这里是并行，而不是并发</p>
<p>这里我们将整个集群并行执行任务的数量称之为并行度</p>
<p>一个作业的并行度取决于框架的默认配置，应用程序也可以在运行过程中动态修改</p>
<h4 id="有向无环图（DAG）"><a href="#有向无环图（DAG）" class="headerlink" title="有向无环图（DAG）"></a>有向无环图（DAG）</h4><p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261558191.png" alt="image-20230126155858119" loading="lazy"></p>
<p><code>DAG（Directed Acyclic Graph）有向无环图</code> 是由点和线组成的拓扑图形，该图形具有方向，不会闭环</p>
<p>这里所谓的有向无环图，并不是真正意义的图形，而是由 Spark 程序直接映射成的数据流的高级抽象模型</p>
<p>简单理解就是将整个程序计算的执行过程用图形表示出来，这样更直观且更便于理解，可以用于表示程序的拓扑结构</p>
<blockquote>
<p>大数据计算引擎框架我们根据使用方式的不同一般会分为四类，其中第一类就是 Hadoop 所承载的 MapReduce，它将计算分为两个阶段，Map 阶段 和 Reduce 阶段 </p>
<p>对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现多个 Job 的串联，以完成一个完整的算法，例如迭代计算，由于这样的弊端，催生了支持 DAG 框架的产生</p>
</blockquote>
<h2 id="Spark-核心编程"><a href="#Spark-核心编程" class="headerlink" title="Spark 核心编程"></a>Spark 核心编程</h2><blockquote>
<p>Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。</p>
<p>三大数据结构分别是： </p>
<ul>
<li>RDD：弹性分布式数据集 </li>
<li>累加器：分布式共享只写变量 </li>
<li>广播变量：分布式共享只读变量</li>
</ul>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261622672.png" alt="image-20230126162247383" loading="lazy"></p>
<p>上图模拟了 Spark 中分布式计算的过程，客户端向服务器发送计算任务</p>
<p>任务拆分：(1, 2, 3, 4) =&gt; (1, 2)  (3, 4)</p>
<p>计算逻辑：n * 2</p>
<h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261628117.png" alt="image-20230126162838979" loading="lazy"></p>
<p><code>RDD（Resilient Distributed Datase）</code>叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型，源码中是一个抽象类</p>
<h4 id="RDD-的特点"><a href="#RDD-的特点" class="headerlink" title="RDD 的特点"></a>RDD 的特点</h4><ul>
<li><p>弹性</p>
<ul>
<li>存储的弹性：内存与磁盘的自动切换</li>
<li>容错的弹性：数据丢失可以自动恢复 </li>
<li>计算的弹性：计算出错重试机制</li>
<li>分片的弹性：可根据需要重新分片</li>
</ul>
</li>
<li><p>分布式：数据存储在大数据集群不同节点上 </p>
</li>
<li><p>数据集：RDD 封装了计算逻辑，并不保存数据</p>
</li>
<li><p>数据抽象：RDD 是一个抽象类，需要子类具体实现 </p>
</li>
</ul>
<ul>
<li>不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变只能产生新的 RDD，在新的 RDD 里面封装计算逻辑 </li>
</ul>
<ul>
<li>可分区、并行计算</li>
</ul>
<h4 id="RDD-与-IO-流的联系"><a href="#RDD-与-IO-流的联系" class="headerlink" title="RDD 与 IO 流的联系"></a>RDD 与 IO 流的联系</h4><h5 id="IO"><a href="#IO" class="headerlink" title="IO"></a>IO</h5><blockquote>
<p>IO 操作体现了装饰者设计模式</p>
</blockquote>
<p>字节流</p>
<p>一个一个字节读取，读一个打印一个</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261712105.png" alt="image-20230126171245898" loading="lazy"></p>
<p>字节流 + 缓冲区</p>
<p>一次读取一个 <code>字节缓冲区</code> 大小的字节数，存满后一起打印</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261711887.png" alt="image-20230126171114657" loading="lazy"></p>
<p>字符流</p>
<p>一次读取一个 <code>字节缓冲区</code> 大小的字节，此大小正好组装成一个字符，再暂存到 <code>字符缓冲区</code>，存满后一起打印</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261716326.png" alt="image-20230126171626972" loading="lazy"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261653967.png" alt="image-20230126165309740" loading="lazy"></p>
<h5 id="RDD-1"><a href="#RDD-1" class="headerlink" title="RDD"></a>RDD</h5><p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301261655898.png" alt="image-20230126165522532" loading="lazy"></p>
<ul>
<li>RDD 的数据处理方式类似于 IO 流，也体现了装饰者设计模式</li>
<li>RDD 的数据只有在调用 collect 方法时，才会真正执行业务逻辑操作，之前的封装均为功能扩展</li>
<li>RDD 不保存数据，但 IO 可以通过缓冲区暂存一部分数据</li>
</ul>
<h4 id="RDD-创建"><a href="#RDD-创建" class="headerlink" title="RDD 创建"></a>RDD 创建</h4><h5 id="从内存中创建-RDD"><a href="#从内存中创建-RDD" class="headerlink" title="从内存中创建 RDD"></a>从内存中创建 RDD</h5><pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token keyword">val</span> sparkConf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"spark"</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> sparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>
<span class="token keyword">val</span> rdd1 <span class="token operator">=</span> sparkContext<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>
    List<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
<span class="token keyword">val</span> rdd2 <span class="token operator">=</span> sparkContext<span class="token punctuation">.</span>makeRDD<span class="token punctuation">(</span>
    List<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
rdd1<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
rdd2<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
sparkContext<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>

<p>Spark 主要提供了两个方法：<code>parallelize</code> 和 <code>makeRDD</code></p>
<p>底层代码中 <code>makeRDD</code> 方法其实就是 <code>parallelize</code> 方法，且大部分时候使用 <code>makeRDD</code> 方法居多</p>
<h5 id="从外存中创建-RDD"><a href="#从外存中创建-RDD" class="headerlink" title="从外存中创建 RDD"></a>从外存中创建 RDD</h5><p>由外部存储系统的数据集创建 RDD 包括：</p>
<ul>
<li>本地的文件系统</li>
<li>所有 Hadoop 支持的数据集，比如 HDFS、HBase 等</li>
</ul>
<pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token keyword">val</span> sparkConf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"spark"</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> sparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>
<span class="token keyword">val</span> fileRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> sparkContext<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"input"</span><span class="token punctuation">)</span>
fileRDD<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
sparkContext<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>

<h5 id="从其他-RDD-创建"><a href="#从其他-RDD-创建" class="headerlink" title="从其他 RDD 创建"></a>从其他 RDD 创建</h5><p>通过一个 RDD 运算完后，再产生新的 RDD</p>
<h5 id="直接创建-RDD（new）"><a href="#直接创建-RDD（new）" class="headerlink" title="直接创建 RDD（new）"></a>直接创建 RDD（new）</h5><p>使用 new 的方式直接构造 RDD，一般由 Spark 框架自身使用</p>
<h4 id="RDD-分区"><a href="#RDD-分区" class="headerlink" title="RDD 分区"></a>RDD 分区</h4><p>默认情况下，Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而<strong>能够并行计算的任务数量</strong>称之为并行度，这个数量可以在构建 RDD 时指定</p>
<p>注意，这里是指<strong>并行执行的任务数量</strong>，并不是指<strong>切分任务的数量</strong></p>
<pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token keyword">val</span> sparkConf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"spark"</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> sparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>
<span class="token keyword">val</span> dataRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">Int</span><span class="token punctuation">]</span> <span class="token operator">=</span>
    sparkContext<span class="token punctuation">.</span>makeRDD<span class="token punctuation">(</span>
        List<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token number">4</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> fileRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span>
    sparkContext<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span>
        <span class="token string">"input"</span><span class="token punctuation">,</span>
        <span class="token number">2</span><span class="token punctuation">)</span>
fileRDD<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
sparkContext<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>

<ul>
<li>读取<strong>内存数据</strong>时，数据可以按照并行度的设定进行数据的分区操作</li>
<li>读取<strong>文件数据</strong>时，数据是按照 Hadoop 文件读取的规则进行切片分区，而切片规则和数据读取规则有差异</li>
</ul>
<h3 id="RDD-算子"><a href="#RDD-算子" class="headerlink" title="RDD 算子"></a>RDD 算子</h3><p>RDD 算子就是 RDD 方法，只是名称不同</p>
<p>RDD 方法分为转换方法和行动方法</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301262300141.png" alt="image-20230126230055836" loading="lazy"></p>
<h4 id="RDD-转换算子"><a href="#RDD-转换算子" class="headerlink" title="RDD 转换算子"></a>RDD 转换算子</h4><p>RDD 根据数据处理方式的不同将算子整体上分为 </p>
<ul>
<li>Value 类型</li>
<li>双 Value 类型</li>
<li>Key-Value 类型</li>
</ul>
<h5 id="Value类型"><a href="#Value类型" class="headerlink" title="Value类型"></a>Value类型</h5><ul>
<li><p>map</p>
<p>将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换</p>
</li>
<li><p>mapPartitions</p>
<p>将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是进行任意处理，也可以是过滤数据</p>
</li>
</ul>
<blockquote>
<p>map 与 mapPartitions 的区别</p>
<p>map 是分区内一个数据一个数据的执行，类似于串行操作</p>
<p>mapPartitions 是以分区为单位进行批处理操作</p>
<p>所以，map 因为类似于串行操作，性能比较低，而 mapPartitions 类似于批处理，性能较高，但是 mapPartitions 会长时间占用内存，可能会导致内存溢出，在内存有限的情况下不推荐使用</p>
</blockquote>
<ul>
<li><p>mapPartitionsWithIndex</p>
<p>与 mapPartitions 类似，只是在处理时可以获取当前分区索引</p>
</li>
<li><p>flatMap</p>
<p>将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射</p>
</li>
<li><p>glom</p>
<p>将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变</p>
</li>
<li><p>groupBy</p>
<p>将数据根据指定的规则进行分组，分区默认不变，但是数据会被打乱重新组合，这样的操作称之为 shuffle</p>
</li>
<li><p>filter</p>
<p>将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃</p>
</li>
<li><p>sample</p>
<p>根据指定的规则从数据集中抽取数据</p>
</li>
<li><p>distinct</p>
<p>将数据集中重复的数据去重</p>
</li>
<li><p>coalesce</p>
<p>根据数据量缩减分区，当存在过多小任务时，可以通过 coalesce 方法收缩合并分区，减少分区的个数，减小任务调度成本</p>
</li>
<li><p>sortBy</p>
<p>用于排序数据</p>
</li>
</ul>
<h5 id="双-Value-类型"><a href="#双-Value-类型" class="headerlink" title="双 Value 类型"></a>双 Value 类型</h5><ul>
<li><p>intersection</p>
<p>对两个 RDD 求交集</p>
</li>
<li><p>union</p>
<p>对两个 RDD 求并集</p>
</li>
<li><p>subtract</p>
<p>对两个 RDD 求差集，以一个 RDD 元素为主，去除两个 RDD 中重复元素，将其他元素保留下来</p>
</li>
<li><p>zip</p>
<p>将两个 RDD 中的元素以键值对的形式进行合并，其中键值对中的 Key 为第 1 个 RDD 中的元素，Value 为第 2 个 RDD 中的相同位置的元素</p>
</li>
</ul>
<h5 id="Key-Value-类型"><a href="#Key-Value-类型" class="headerlink" title="Key - Value 类型"></a>Key - Value 类型</h5><ul>
<li><p>partitionBy</p>
<p>将数据按照指定 Partitioner 重新进行分区，Spark 默认的分区器是 HashPartitioner</p>
</li>
<li><p>reduceByKey</p>
<p>可以将数据按照相同的 key 对 value 进行聚合</p>
</li>
<li><p>groupByKey</p>
<p>将数据源的数据根据 key 对 value 进行分组</p>
</li>
<li><p>aggregateByKey</p>
<p>将数据根据不同的规则进行分区内计算和分区间计算</p>
</li>
<li><p>foldByKey</p>
<p>当分区内计算规则和分区间计算规则相同时，aggregateByKey 就可以简化为 foldByKey</p>
</li>
<li><p>combineByKey</p>
<p>最通用的对 key-value 型 RDD 进行聚集操作的聚集函数</p>
</li>
<li><p>sortByKey</p>
<p>在一个 (K,V) 的 RDD 上调用，K 必须实现 Ordered 接口(特质)，返回一个按照 key 进行排序的 RDD</p>
</li>
<li><p>join</p>
<p>在类型为 (K,V) 和 (K,W) 的 RDD 上调用，返回一个相同 key 对应的所有元素连接在一起的 (K,(V,W)) 的 RDD</p>
</li>
</ul>
<h4 id="RDD-行动算子"><a href="#RDD-行动算子" class="headerlink" title="RDD 行动算子"></a>RDD 行动算子</h4><ul>
<li><p>reduce</p>
<p>聚集 RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据</p>
</li>
<li><p>collect</p>
<p>在驱动程序中，以数组 Array 的形式返回数据集的所有元素</p>
</li>
<li><p>count</p>
<p>返回 RDD 中元素的个数</p>
</li>
<li><p>first</p>
<p>返回 RDD 中的第一个元素</p>
</li>
<li><p>take</p>
<p>返回一个由 RDD 的前 n 个元素组成的数组</p>
</li>
<li><p>takeOrdered</p>
<p>返回该 RDD 排序后的前 n 个元素组成的数组</p>
</li>
<li><p>aggregate</p>
<p>分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合</p>
</li>
<li><p>fold</p>
<p>折叠操作，aggregate 的简化版操作</p>
</li>
<li><p>countByKey</p>
<p>统计每种 key 的个数</p>
</li>
<li><p>save 相关</p>
<p>将数据保存到不同格式的文件中</p>
</li>
<li><p>foreach</p>
<p>分布式遍历 RDD 中的每一个元素，调用指定函数</p>
</li>
</ul>
<h3 id="RDD-序列化"><a href="#RDD-序列化" class="headerlink" title="RDD 序列化"></a>RDD 序列化</h3><p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202301262351595.png" loading="lazy"></p>
<p>从计算的角度，算子外的代码都是在 Driver 端执行，算子内的代码都是在 Executor 端执行</p>
<p>那么在 Scala 的函数式编程中，就会导致算子内经常会用到算子外的数据，如果使用的算子外的数据无法序列化，就意味着无法<strong>通过网络</strong>传值给 Executor 端执行，发生错误</p>
<p>所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作称之为闭包检测</p>
<h4 id="Kryo-序列化框架"><a href="#Kryo-序列化框架" class="headerlink" title="Kryo 序列化框架"></a>Kryo 序列化框架</h4><p>Java 的序列化能够序列化任何类，但是生成的字节多，序列化后对象比较大</p>
<p>Spark 出于性能的考虑，Spark2.0 开始支持另外一种 Kryo 序列化机制</p>
<p>Kryo 速度是 Serializable 的 10 倍，当 RDD 在 Shuffle 数据的时候，简单数据类型、数组和字符串类已经在 Spark 内部使用 Kryo 来序列化</p>
<h3 id="RDD-依赖关系"><a href="#RDD-依赖关系" class="headerlink" title="RDD 依赖关系"></a>RDD 依赖关系</h3><p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091705504.png" alt="image-20230209170526155" loading="lazy"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091705344.png" alt="image-20230209170536238" loading="lazy"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091712456.png" alt="image-20230209171241203" loading="lazy"></p>
<p>RDD 是不会保存数据的，为了提供容错性，需要将 RDD 的依赖关系保存下来</p>
<p>一旦发生错误，可根据血缘关系重新读取数据并计算</p>
<h4 id="依赖关系"><a href="#依赖关系" class="headerlink" title="依赖关系"></a>依赖关系</h4><p><code>val rdd1 = rdd.map(_ * 2)</code></p>
<p>相邻两个 RDD 称为依赖关系</p>
<h4 id="血缘关系"><a href="#血缘关系" class="headerlink" title="血缘关系"></a>血缘关系</h4><p>RDD1、RDD2、RDD3、RDD4 称为血缘关系</p>
<h4 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h4><p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091731919.png" alt="image-20230209173155612" loading="lazy"></p>
<p>窄依赖表示每一个父 RDD 的 Partition 最多被子 RDD 的一个 Partition 使用</p>
<p>窄依赖可类比独生子女</p>
<h4 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h4><p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091732650.png" alt="image-20230209173250322" loading="lazy"></p>
<p>宽依赖表示同一个父 RDD 的 Partition 被多个子 RDD 的 Partition 依赖，会引起 Shuffle</p>
<p>宽依赖可类比多生</p>
<h3 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h3><p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091753853.png" alt="image-20230209175349601" loading="lazy"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091754211.png" alt="image-20230209175429945" loading="lazy"></p>
<p>RDD 不存储数据，如果一个 RDD 需要重复使用，那么就需要<strong>从头开始</strong>再次执行来获取数据</p>
<p>在数据执行任务较长，或数据比较重要的场合可以使用持久化</p>
<h4 id="RDD-Cache-缓存"><a href="#RDD-Cache-缓存" class="headerlink" title="RDD Cache 缓存"></a>RDD Cache 缓存</h4><p>RDD 通过 Cache 或者 Persist 方法将前面的计算结果进行缓存，默认情况下会把数据缓存在 JVM 的堆内存中</p>
<p>缓存有可能丢失或者由于内存不足而被删除，RDD 的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行</p>
<pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token comment">// cache 操作会增加血缘关系，不改变原有的血缘关系</span>
println<span class="token punctuation">(</span>wordToOneRdd<span class="token punctuation">.</span>toDebugString<span class="token punctuation">)</span>
<span class="token comment">// 数据缓存</span>
wordToOneRdd<span class="token punctuation">.</span>cache<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">// 可以更改存储级别</span>
mapRdd<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>StorageLevel<span class="token punctuation">.</span>MEMORY_AND_DISK_2<span class="token punctuation">)</span></code></pre>

<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302091741399.png" alt="image-20230209174139303" loading="lazy"></p>
<h4 id="RDD-CheckPoint-检查点"><a href="#RDD-CheckPoint-检查点" class="headerlink" title="RDD CheckPoint 检查点"></a>RDD CheckPoint 检查点</h4><p>检查点其实就是将 RDD 中间结果写入磁盘</p>
<p>血缘依赖过长会造成容错成本过高，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销</p>
<h4 id="缓存和检查点区别"><a href="#缓存和检查点区别" class="headerlink" title="缓存和检查点区别"></a>缓存和检查点区别</h4><ul>
<li><p>Cache 缓存只是将数据保存起来，不切断血缘依赖</p>
<p>Checkpoint 检查点切断血缘依赖</p>
</li>
<li><p>Cache 缓存的数据通常存储在内存，可靠性低</p>
<p>Checkpoint 的数据通常存储在 HDFS 等容错、高可用的文件系统，可靠性高</p>
</li>
</ul>
<h3 id="RDD-分区器"><a href="#RDD-分区器" class="headerlink" title="RDD 分区器"></a>RDD 分区器</h3><p>Spark 目前支持 Hash 分区、Range 分区、用户自定义分区，默认为 Hash 分区</p>
<p>只有 Key-Value 类型的 RDD 才有分区器，非 Key-Value 类型的 RDD 分区的值是 None</p>
<h4 id="Hash-分区"><a href="#Hash-分区" class="headerlink" title="Hash 分区"></a>Hash 分区</h4><p>对于给定的 key，计算其 hashCode,并除以分区个数取余</p>
<h4 id="Range-分区"><a href="#Range-分区" class="headerlink" title="Range 分区"></a>Range 分区</h4><p>将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而 且分区间有序</p>
<h3 id="RDD-文件读取与保存"><a href="#RDD-文件读取与保存" class="headerlink" title="RDD 文件读取与保存"></a>RDD 文件读取与保存</h3><p>Spark 的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统</p>
<p>文件格式分为：text 文件、csv 文件、sequence 文件以及 Object 文件</p>
<p>文件系统分为：本地文件系统、HDFS、HBASE 以及数据库</p>
<h4 id="text-文件"><a href="#text-文件" class="headerlink" title="text 文件"></a>text 文件</h4><pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token comment">// 读取输入文件</span>
<span class="token keyword">val</span> inputRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"input/1.txt"</span><span class="token punctuation">)</span>
<span class="token comment">// 保存数据</span>
inputRDD<span class="token punctuation">.</span>saveAsTextFile<span class="token punctuation">(</span><span class="token string">"output"</span><span class="token punctuation">)</span></code></pre>

<h4 id="sequence-文件"><a href="#sequence-文件" class="headerlink" title="sequence 文件"></a>sequence 文件</h4><p>SequenceFile 文件是 Hadoop 用来存储二进制形式的 key-value 而设计的一种平面文件 (Flat File)</p>
<pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token comment">// 保存数据为 SequenceFile</span>
dataRDD<span class="token punctuation">.</span>saveAsSequenceFile<span class="token punctuation">(</span><span class="token string">"output"</span><span class="token punctuation">)</span>
<span class="token comment">// 读取 SequenceFile 文件</span>
sc<span class="token punctuation">.</span>sequenceFile<span class="token punctuation">[</span><span class="token builtin">Int</span><span class="token punctuation">,</span><span class="token builtin">Int</span><span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"output"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span></code></pre>

<h4 id="object-对象文件"><a href="#object-对象文件" class="headerlink" title="object 对象文件"></a>object 对象文件</h4><p>对象文件是将对象序列化后保存的文件，采用 Java 的序列化机制</p>
<pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token comment">// 保存数据</span>
dataRDD<span class="token punctuation">.</span>saveAsObjectFile<span class="token punctuation">(</span><span class="token string">"output"</span><span class="token punctuation">)</span>
<span class="token comment">// 读取数据</span>
sc<span class="token punctuation">.</span>objectFile<span class="token punctuation">[</span><span class="token builtin">Int</span><span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"output"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span></code></pre>

<h3 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h3><p>累加器用于把 Executor 的变量数据聚合到 Driver</p>
<p>对于累加器，在 Driver 中定义的变量，每个 Executor 的 Task 都会进行复制得到一个副本，Task 更新变量副本后，会返回 Driver 进行合并</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302101720543.png" alt="image-20230210172002330" loading="lazy"></p>
<pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token keyword">val</span> rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>makeRDD<span class="token punctuation">(</span>List<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">// 声明累加器</span>
<span class="token keyword">var</span> sum <span class="token operator">=</span> sc<span class="token punctuation">.</span>longAccumulator<span class="token punctuation">(</span><span class="token string">"sum"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
rdd<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>
    num <span class="token keyword">=></span> <span class="token punctuation">&#123;</span>
        <span class="token comment">// 使用累加器</span>
        sum<span class="token punctuation">.</span>add<span class="token punctuation">(</span>num<span class="token punctuation">)</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">)</span>
<span class="token comment">// 获取累加器的值</span>
println<span class="token punctuation">(</span><span class="token string">"sum = "</span> <span class="token operator">+</span> sum<span class="token punctuation">.</span>value<span class="token punctuation">)</span></code></pre>

<h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><p>闭包数据都是以 Task 为单位进行发送的，这样可能导致一个 Executor 中含有大量重复数据，浪费大量内存</p>
<p>Executor 本质上是一个 JVM，启动时会自动为其分配内存，因此可以将闭包数据放在该内存中，达到共享目的</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302101721345.png" alt="image-20230210172135194" loading="lazy"></p>
<pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token keyword">val</span> rdd1 <span class="token operator">=</span> sc<span class="token punctuation">.</span>makeRDD<span class="token punctuation">(</span>List<span class="token punctuation">(</span> <span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"b"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"c"</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"d"</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> list <span class="token operator">=</span> List<span class="token punctuation">(</span> <span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"b"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"c"</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"d"</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span> <span class="token punctuation">)</span>
<span class="token comment">// 声明广播变量</span>
<span class="token keyword">val</span> broadcast<span class="token operator">:</span> Broadcast<span class="token punctuation">[</span>List<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> sc<span class="token punctuation">.</span>broadcast<span class="token punctuation">(</span>list<span class="token punctuation">)</span>
<span class="token keyword">val</span> resultRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token builtin">Int</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> rdd1<span class="token punctuation">.</span>map <span class="token punctuation">&#123;</span>
    <span class="token keyword">case</span> <span class="token punctuation">(</span>key<span class="token punctuation">,</span> num<span class="token punctuation">)</span> <span class="token keyword">=></span> <span class="token punctuation">&#123;</span>
        <span class="token keyword">var</span> num2 <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token comment">// 使用广播变量</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> v<span class="token punctuation">)</span> <span class="token keyword">&lt;-</span> broadcast<span class="token punctuation">.</span>value<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
            <span class="token keyword">if</span> <span class="token punctuation">(</span>k <span class="token operator">==</span> key<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
                num2 <span class="token operator">=</span> v
            <span class="token punctuation">&#125;</span>
        <span class="token punctuation">&#125;</span>
        <span class="token punctuation">(</span>key<span class="token punctuation">,</span> <span class="token punctuation">(</span>num<span class="token punctuation">,</span> num2<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span></code></pre>



<h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><p>Spark SQL 是 Spark 用于处理结构化数据（structured data）的模块</p>
<h3 id="Spark-SQL-与-Hive"><a href="#Spark-SQL-与-Hive" class="headerlink" title="Spark SQL 与 Hive"></a>Spark SQL 与 Hive</h3><p>Hive 是早期唯一运行在 Hadoop 上的 SQL-on-Hadoop 工具，但 MapReduce 计算过程中需要进行大量磁盘 I/O，降低了运行效率</p>
<p>Shark 是 SparkSQL 的前身，它的出现使 SQL-on-Hadoop 的性能比 Hive 有了 10-100 倍的提高，但随着 Spark 的发展，Shark 对 Hive 过于依赖，制约了 Spark 的发展</p>
<p>SparkSQL 抛弃了原有 Shark 的代码，吸取了其中的优点，同时摆脱了对 Hive 的依赖性，在数据兼容、性能优化、组件扩展方面有极大提高</p>
<h3 id="Spark-SQL-特点"><a href="#Spark-SQL-特点" class="headerlink" title="Spark SQL 特点"></a>Spark SQL 特点</h3><h4 id="易整合"><a href="#易整合" class="headerlink" title="易整合"></a>易整合</h4><p>便捷地整合了 SQL 查询和 Spark 编程</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302132144518.png" alt="image-20230213214353200" loading="lazy"></p>
<h4 id="数据访问方式统一"><a href="#数据访问方式统一" class="headerlink" title="数据访问方式统一"></a>数据访问方式统一</h4><p>使用相同的方式连接不同的数据源</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302132144369.png" alt="image-20230213214426269" loading="lazy"></p>
<h4 id="标准数据连接"><a href="#标准数据连接" class="headerlink" title="标准数据连接"></a>标准数据连接</h4><p>通过 JDBC 或者 ODBC 来连接</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302132144548.png" alt="image-20230213214456341" loading="lazy"></p>
<h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><h4 id="Spark-1-x"><a href="#Spark-1-x" class="headerlink" title="Spark 1.x"></a>Spark 1.x</h4><p>在早期版本 Spark 1.x 中，DataFrame 类似于传统数据库中的二维表格</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302132214130.png" alt="image-20230213221449968" loading="lazy"></p>
<p>左侧的 RDD[Person]虽然以 Person 为类型参数，但 Spark 框架本身不了解 Person 类的内部结构</p>
<p>右侧的 DataFrame 多了数据的结构信息，即 schema，可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么</p>
<h4 id="Spark-2-x"><a href="#Spark-2-x" class="headerlink" title="Spark 2.x"></a>Spark 2.x</h4><p>但在新版本 Spark 2.x 中，DataFrame=DataSet[Row]，即DataFrame 每一行是 <strong>Row 类型</strong>，但每一行究竟有哪些字段，各个字段又是什么类型都无从得知，即每一列的值无法直接访问，只能用 <code>getAS</code> 方法得到各字段的具体值</p>
<pre class="language-scala" data-language="scala"><code class="language-scala">testDataFrame<span class="token punctuation">.</span>foreach<span class="token punctuation">&#123;</span>
  line <span class="token keyword">=></span>
    <span class="token keyword">val</span> col1<span class="token operator">=</span>line<span class="token punctuation">.</span>getAs<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"col1"</span><span class="token punctuation">)</span>
    <span class="token keyword">val</span> col2<span class="token operator">=</span>line<span class="token punctuation">.</span>getAs<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"col2"</span><span class="token punctuation">)</span>
<span class="token punctuation">&#125;</span></code></pre>

<h4 id="DataFrame-查询优化"><a href="#DataFrame-查询优化" class="headerlink" title="DataFrame 查询优化"></a>DataFrame 查询优化</h4><p>DataFrame 与 RDD 都是懒执行的，但 DataFrame 性能比 RDD 高，因为 Spark SQL 具有<strong>查询优化器</strong>：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302132156508.png" alt="image-20230213215625359" loading="lazy"></p>
<p>图中构造了两个 DataFrame，先将他们 join 然后进行 filter 操作</p>
<p>如果原封不动地执行这个执行计划，最终的执行效率是不高的，因为 join 是一个代价较大的操作，也可能会产生一个较大的数据集</p>
<p>如果我们能将 filter 延迟到 join 之后执行，先对 DataFrame 进行过滤，再 join 过滤后的较小的结果集，便可以有效缩短执行时间</p>
<p>而 Spark SQL 的查询优化器正是这样做的，简而言之，就是将<strong>高成本</strong>的操作替换为<strong>低成本</strong>操作的过程</p>
<blockquote>
<p>关于懒执行：</p>
<p>Spark算子主要划分为两类：transformation 和 action ，并且只有action算子触发的时候才会真正执行任务</p>
<p>常用的算子map、flatMap、filter都是 transformation 算子</p>
<p>而collect、count、saveAsTextFile、countByKey、foreach则为action算子</p>
<p><strong>为什么 Spark 任务只有在调用 action 算子的时候，才会真正执行呢？</strong></p>
<p>假如 transformation 算子直接触发 Spark 任务：</p>
<ul>
<li>导致 map 执行完了要立即输出，数据也必然要落地（内存和磁盘）</li>
<li>对于 map 任务的生成、调度、执行以及彼此之间的 RPC 通信等，当涉及大数据量时，会很影响性能</li>
</ul>
<p>MapReduce 正是因为中间结果需要落地，导致性能低下</p>
<p>Spark 只有调用 action 算子时才会真正执行任务，这是相对于 MapReduce 的优化之一</p>
</blockquote>
<h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>这里以新版本 Spark 2.x 为例</p>
<p><code>DataFrame=DataSet[Row]</code></p>
<p>DataFrame 也可以称为 DataSet[Row]，每一行的类型都是 <strong>Row</strong>，但每一行有哪些字段，各字段是什么类型都无法得知</p>
<p>DataSet 不同于 DataFrame，在定义了 case class 后可以直接获取每列的值</p>
<pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token keyword">case</span> <span class="token keyword">class</span> Coltest<span class="token punctuation">(</span>col1<span class="token operator">:</span><span class="token builtin">String</span><span class="token punctuation">,</span>col2<span class="token operator">:</span><span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token keyword">extends</span> Serializable <span class="token comment">//定义字段名和类型</span>

<span class="token keyword">val</span> test<span class="token operator">:</span> Dataset<span class="token punctuation">[</span>Coltest<span class="token punctuation">]</span><span class="token operator">=</span>rdd<span class="token punctuation">.</span>map<span class="token punctuation">&#123;</span>line<span class="token keyword">=></span>
      Coltest<span class="token punctuation">(</span>line<span class="token punctuation">.</span>_1<span class="token punctuation">,</span>line<span class="token punctuation">.</span>_2<span class="token punctuation">)</span>
    <span class="token punctuation">&#125;</span><span class="token punctuation">.</span>toDS

test<span class="token punctuation">.</span>map<span class="token punctuation">&#123;</span>
      line<span class="token keyword">=></span>
        println<span class="token punctuation">(</span>line<span class="token punctuation">.</span>col1<span class="token punctuation">)</span>
        println<span class="token punctuation">(</span>line<span class="token punctuation">.</span>col2<span class="token punctuation">)</span>
    <span class="token punctuation">&#125;</span></code></pre>

<h4 id="DataSet-gt-DataFrame"><a href="#DataSet-gt-DataFrame" class="headerlink" title="DataSet -&gt; DataFrame"></a>DataSet -&gt; DataFrame</h4><p>DataSet 封装成 Row 类型后，可转为 DataFrame</p>
<pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token keyword">import</span> <span class="token namespace">spark<span class="token punctuation">.</span>implicits<span class="token punctuation">.</span></span>_

<span class="token keyword">val</span> testDF <span class="token operator">=</span> testDS<span class="token punctuation">.</span>toDF</code></pre>

<h4 id="DataFrame-gt-DataSet"><a href="#DataFrame-gt-DataSet" class="headerlink" title="DataFrame -&gt; DataSet"></a>DataFrame -&gt; DataSet</h4><p>DataFrame 给出每列的具体类型，使用 as 方法，可转为 DataSet</p>
<pre class="language-scala" data-language="scala"><code class="language-scala"><span class="token keyword">import</span> <span class="token namespace">spark<span class="token punctuation">.</span>implicits<span class="token punctuation">.</span></span>_

<span class="token keyword">case</span> <span class="token keyword">class</span> Coltest<span class="token punctuation">(</span>col1<span class="token operator">:</span><span class="token builtin">String</span><span class="token punctuation">,</span>col2<span class="token operator">:</span><span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token keyword">extends</span> Serializable <span class="token comment">//定义字段名和类型</span>
<span class="token keyword">val</span> testDS <span class="token operator">=</span> testDF<span class="token punctuation">.</span>as<span class="token punctuation">[</span>Coltest<span class="token punctuation">]</span></code></pre>

<h3 id="Spark-Session"><a href="#Spark-Session" class="headerlink" title="Spark Session"></a>Spark Session</h3><p>在以往 Spark Core 中，若要执行应用程序，需要先创建上下文对象 SparkContext</p>
<p>而 Spark SQL 对其进行了封装，称为 SparkSession，所以实际上的计算是由内部 SparkContext 完成的</p>
<p>Spark Session 是 Spark SQL 查询的起点</p>
<h3 id="用户自定义函数-UDF"><a href="#用户自定义函数-UDF" class="headerlink" title="用户自定义函数 UDF"></a>用户自定义函数 UDF</h3><p>用户可以通过 <code>spark.udf</code> 创建自定义函数，实现自定义功能</p>
<pre class="language-scala" data-language="scala"><code class="language-scala">spark<span class="token punctuation">.</span>udf<span class="token punctuation">.</span>register<span class="token punctuation">(</span><span class="token string">"addName"</span><span class="token punctuation">,</span><span class="token punctuation">(</span>x<span class="token operator">:</span><span class="token builtin">String</span><span class="token punctuation">)</span><span class="token keyword">=></span> <span class="token string">"Name:"</span><span class="token operator">+</span>x<span class="token punctuation">)</span></code></pre>

<h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><p>Spark Streaming 用于流式数据的处理，支持的数据源很多，例如 Kafka、简单 TCP 套接字等</p>
<p>与 Spark 基于 RDD 类似，Spark Streaming 基于 DStream，DStream 是随时间推移收到的数据序列</p>
<p>DStream 内部每个时间间隔收到的数据是作为 RDD 存在，而 DStream 是由这些 RDD 组成的序列，即离散化</p>
<p>总之，DStream 是对 RDD 在实时数据处理场景的封装</p>
<blockquote>
<p><strong>流式处理 和 批量处理</strong></p>
<p>从数据处理的方式角度</p>
<ul>
<li>流式处理：接收一条数据，立即进行处理</li>
<li>批量处理：接收一条数据，先暂存，收集一批数据后一起处理</li>
</ul>
<p><strong>实时处理 和 离线处理</strong></p>
<p>从数据处理的延迟长短角度</p>
<ul>
<li>实时处理：毫秒</li>
<li>离线处理：小时、天</li>
</ul>
</blockquote>
<p>而 Spark Streaming 是准实时（秒、分钟）的数据处理框架</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302191540611.png" alt="image-20230219154000410" loading="lazy"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302191540945.png" alt="image-20230219154017818" loading="lazy"></p>
<h3 id="背压机制"><a href="#背压机制" class="headerlink" title="背压机制"></a>背压机制</h3><p><img src="https://cdn.jsdelivr.net/gh/Rayucan/imageCloud/data/202302191542019.png" alt="image-20230219154236720" loading="lazy"></p>
<p>Spark Streaming 处理任务采用生产者-消费者模式，接收器（或采集器，Receiver）接收到数据后，由 Driver 发送给 Executor 工作节点处理数据</p>
<p>此时可能会出现一些问题，如接收器接收太快，工作节点来不及处理数据，会造成数据积压，或工作节点处理数据太快，造成资源浪费</p>
<p>为了协调接收速率和处理速率，Spark Streaming 采用背压机制，可根据作业执行情况动态调整 Receiver 接收速率</p>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>Rayucan</li><li class="post-copyright-link"><strong>Post link: </strong><a href="http://example.com/2023/01/07/Spark/" title="Spark">http://example.com/2023/01/07/Spark/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> unless otherwise stated.</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2023/02/18/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" rel="prev" title="推荐系统"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">推荐系统</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2023/01/01/Scala/" rel="next" title="Scala"><span class="post-nav-text">Scala</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>要不要和我说些什么？</span><br></div></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2020 – 2023 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> Rayucan</span></div><div class="powered"><span>Powered by <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> v5.4.0</span><span class="footer-separator">|</span><span>Theme - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.6.1</span></div></footer><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></div></body></html>